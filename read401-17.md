# Web Scraping

## What is Web Scraping

is used for extracting data from websites. The web scraping software may directly access the World Wide Web using the Hypertext Transfer Protocol or a web browser.

## Steps to do Web Scraping:

### 1.Inspecting the Website:

1.right click and click on “Inspect”, you will see code HTML
2.Notice that on the top left of the console, there is an arrow symbol.
3.all the .txt files are inside the `<a>` tag.
 like this:
    ```
    <a href=”data/nyct/turnstile/turnstile_180922.txt”>Saturday, September 22, 2018</a>
    ```
    
4.Python Code:
We start by importing the following libraries.
```
import requests
import urllib.request
import time
from bs4 import BeautifulSoup
```
we set the url to the website and access the site with our requests library.

```
url = 'http://web.mta.info/developers/turnstile.html'
response = requests.get(url)
```
Next we parse the html with BeautifulSoup
```
soup = BeautifulSoup(response.text, “html.parser”)
```
We use the method .findAll to locate all of our `<a>` tags.

```
soup.findAll('a')

This code gives us every line of code that has an <a> tag.
```
let’s extract the actual link that we want. Let’s test out the first link.

```
one_a_tag = soup.findAll(‘a’)[38]
link = one_a_tag[‘href’]

This code saves the first text file
```
use our urllib.request library to download this file path to our computer.

```
download_url = 'http://web.mta.info/developers/'+ link
urllib.request.urlretrieve(download_url,'./'+link[link.find('/turnstile_')+1:])
```

using this code to avoid getting flagged as a spammer and to make the website knowig we are not spamming:

```
time.sleep(1)
```

## How can websites detect and block web scraping?

1. Unusual traffic/high download rate especially from a single client/or IP address within a short time span.

2. Repetitive tasks performed on the website in the same browsing pattern – based on an assumption that a human user won’t perform the same repetitive tasks all the time.

3. Checking if you are real browser – A simple check is to try and execute javascript. Smarter tools can go a lot more and check your Graphic cards and CPUs to make sure you are coming from real browser.

4. Detection through honeypots – these honeypots are usually links which aren’t visible to a normal user but only to a spider. When a scraper/spider tries to access the link, the alarms are tripped.
